Title: LLaMA: Open and Efficient Foundation Language Models

Section headers:
- 1. Introduction
- 2. Model architecture details
- 3. Training methods
- 4. Results and analysis
- 5. Conclusion and future work

Body text:
- We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters.
- We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets.

Figure/Tables with captions:
- Figure 1: A comparison between different foundation language models' performance on trillions of tokens.

Page headers/footers:
- Meta AI
- https://github.com/facebookresearch/llama